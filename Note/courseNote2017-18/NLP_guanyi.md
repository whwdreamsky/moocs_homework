# NLP

> the note for nlp class from guanyi in hit 

课程的最终任务是完成一个音字转换系统

1. 统计语言模型中最重要的两个模型

- 最大熵模型
  - 用于命名实体识别
  - 词法分析

- 隐马尔科夫模型

  条件随机场也是必须要了解的

2. 需要整理之前neubig 学习的内容,并实现现在最主流的方法,一是了解NLP 最为主要的任务,二是也可以作为自己的小工具,三是可以自己写一个博客记录实现过程中的收获,四是当然锻炼编程能力





2. **N-gram 语言模型**

   - 噪声信道模型：通过有噪声的输出信号试图恢复输入信号,这其实这个和语音识别类似，信息源是声音，想要恢复的是文字。

   - 应用：手写识别，翻译，音字转换

      例如词性标注：信源是P(T) 是生成词性标注序列

   - N-gram 模型定义

   - 未登录词的平滑

     - 拉普拉斯定律：加一平滑法
     - Good-Turing估计：

   - 评价指标：==弄清楚这几个指标的推导==

      困惑度

      交叉熵

      信息熵

3. 动态、自适应、基于缓存的语言模型：能够根据词在局部文本中出现情况动态地调整语言模型中的概率分布数据的语言模型

4. 最大熵模型

   **精研这个模型**

   论文

5. 实践

   做一个词性标注器


***

### 分词

Zipf 定律，在大规模真实文本中，少量高频词(语言单位)占有很高的比例，对于绝大多数词汇，无论随着语料库的增加，仍然出现的很少，甚至根本不会出现，这种现象称为数据稀疏性问题。

数据稀疏性可以采用数据平滑的方法解决，使用 lapace ,good-turing,lidstone 的算法解决，通常是分子加一，分母加词表大小的数。

正向最大匹配分词：把最大符号串与词典中的单词条目相匹配，如果不能匹配，就削掉一个汉字继续匹配，直到在词典中找到相应的单词为止。匹配的方向是从右向左

### 马尔科夫(Markov)模型

- n-gram 与马尔科夫模型

  N-gram是 **N-1 阶的马尔科夫过程**，其实马尔科模型

- 马儿科夫模型

- 隐马尔科夫模型

  - HMM 形式化定义，HMM是一个五元组 (S, K, Π, A, B) ，其中 S是状态的集合，K是输出字符的集合，  $\pi$是初始状态的概率，A是状态转移的概率。B是状态转移时输出字符的概率。

    例如 使用HMM 进行词性标注

    S 状态集合：词性集合

    K 输出集合：词汇集合

    $\pi$:其实词性标记t 的概率

    $a_{ij}$:从词性标记ti 到ti+1 的转移概率

    $B_{jk}$:词性tj 对应wk 的发射概率

  三个基本问题：

  1. 给定一个模型，高效的计算输出字符序列的概率

  2. 给定输出序列 和模型，给出最大概率的状态序列

     解码

  3. 给定输入字符的序列O，如何调整模型参数使得生成这个序列的概率最大

  > 对3理解是训练得到这个模型

- 维特比算法

  马尔科夫模型是

  - 设是随机变量序列，其中每个随机变量的取值在有限集，称为状态空间




### 句法分析

难点： 

1. 语义歧义：一个句子有几种句法分析的结果
2. 词类和句法成分不存在一一对应

###chomsky 体系

####上下文无关文法

推导 ： S→NP VP →John V NP →John V NP PP →John ate fish P NP →John ate fish with bone

句法树：

PCFG 

基本假设：

1. 位置无关
2. 上下文无关
3. 祖先无关