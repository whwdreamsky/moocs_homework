# Machine Learning 

### 决策树

- 决策树ID3算法文字描述，要加入过拟合

  ![书上的描述](/Users/oliver/Desktop/AI_photo/ml_1.png)

  ​

#### 规则学习

基本概念： 选择子(属性x 的值为A，这样原子规则)，普化，特化，一致，完备

一致：只覆盖正例不覆盖反例

完备：覆盖所有正例(当然这里也可能覆盖负例)

GS 算法: 按照以下两个原则生成规则

1. 优先选择属性值为正覆盖最多的属性(值)，作为规则
2. 1 下存在多个属性，优先选择覆盖负例数少的

>  ID3 指的是 “示例”学习，而GS 是规则学习，但是我觉得都是产生规则，不断进行划分的啊 
>
>  规则学习的意思就是从数据中提取一系列规则的析取，[x1=A1] V [x2=A2]
>
>  GS 是针对一个目标，产生一系列规则，如果不是正反的布尔属性，就把每个目标的值当做正例

![](/Users/oliver/Desktop/AI_photo/gs_machinelearning.png)

> 这个算法的问题是第五步，PE 有空集的可能吗，如果存在请假设，可能存在，就是在3，4步骤下，反例被完全排除，但是仍有部分正例也不满足条件，这是需要重新特征选择的过程，注意这里的F 是合取，也就是说两个并列的规则，注意CPX 是合取
>
> 相对于之前的概率模型，这个模型最大的问题是如果存在一个异常点
>
> ~~可能整个模型计算不出来规则，可能没有一个规则能全部覆盖正例子~~ : 这个观点是错误的，看到第五步，只更新了正例，负例还是那么多，因此保证了可以通过多个 规则的析取，获得正例和反例的完全划分，其实GS 算法每一步都是在生成一个规则尽可能的包含正例
>
> -这个F 的意思是？最后的规则式
>
> 例子见PPT

- AQ 算法：其实AQ和GS 有相似之处，就是通过生成规则不断对正例覆盖，每次生成一个规则，如果一个规则不能完全覆盖，就单独拿出没有覆盖的正例，就再生成下一个规则，与之前的规则析取，直到所有正例都被析取规则覆盖，区别在于GS 生成规则的方式是通过属性最大覆盖，而AQ 通过 ”一致性” 选择

  ![](/Users/oliver/Desktop/AI_photo/aq.1.png)

  ![](/Users/oliver/Desktop/AI_photo/aq.3.png)

  > 这里的SOLUTION 表是干什么的？
  >
  > 按照什么标准保留两个D
  >
  > 如果consitent 中一致的个数不够怎么办，不到m 个

  ==例子==： 感觉只有看例子才能真正明白 ：）

  ![](/Users/oliver/Desktop/AI_photo/aq.2.png)

- ==扩张矩阵==

  针对每个正例生成一个面向所有反例的矩阵，每个正例对应反例相同的设为死元素为*,不同就保留反例中的值

  AEI 算法

  优先选择“最大公共元素”，就是优先选择出现次数最多的公共元素在通路上

  广义扩张

  给公式和反例NE

- ==最小属性子集==

  > 完全不明白这个在说什么

>  主要总结见machineLearning 下决策树的总结，这里只关注考点

### 概念学习(concept learning)

概念学习指的是

> 反正大概意思就是通过给出的特殊训练实例给出一般概念，跟之前决策树相同也就是根据实例得到规则

教材第二章

1. 一些基本概念：假设空间，假设函数H,目标函数，

2. Find-S 算法：只对正例进行归纳，由最特殊不断的放宽条件

   ~~~python
   #1.将h 初始化为H中最特殊假设,也即是全部为"空集"(一种都不取)
   #2.对每个正例，若h(x) != c(x),用满足x 的 h 极小普化式代替h
   #3.输出假设H
   ~~~

   > 现在才明白，Find-S 是  Special to General , 有点怀疑那个最小属性值的题指的是Find-S
   >
   > 缺陷：
   >
   > 1. 无法知道什么时候学习概念完成
   >
   >    疑问，所有的正例都满足不就学完了吗
   >
   > 2. 当数据是“不一致”的时候无法判断
   >
   >    也不太懂

3. 列表后消除算法

   大概的意思就是穷举所有可能的假设，然后遍历数据集，把不满足假设的条目删除

4. ==候选消除算法==（考第一题重要)

   这里G 中是泛化程度的上界，从最广泛的形式而来，G 包含正例，同时"恰好"不包含反例，S是“恰好”包含正例，是泛化能力的下界，当正例时，从G 中移除不一致，更多的是普化S，普化过程中如果S 比G  更普通则移除普化，同理当反例时，从S 中移除“不一致”(反例不一致，指的是确保不包含反例)，主要对G特化，让G 不包含反例，这里用极小特化，同时如果G 中的假设不包含S 的删除，也就是一定要记住，G 永远是包含S的

   ![](/Users/oliver/Desktop/AI_photo/候选消除.png)

### 人工神经网络

- 反向传播

  对有一个隐层的神经网络反向传播的推导

  1. 初始化所有要学习的权值

  ![](/Users/oliver/Desktop/AI_photo/bp.png)

- 梯度下降: 也就是一种求解函数最优化的方法，基础的梯度下降是批量的梯度下降，先对遍历所有示例，积累梯度，再更新权值，而随机梯度下降是直接每个例子更新梯度，即把4.9删除，把4.8 改为 $w_i=w_i+\eta(t-o)x_i$。

![](/Users/oliver/Desktop/AI_photo/GD.png)

> 反向传播和梯度下降的关系，梯度下降是为了找到损失函数的最优解，而遇到问题是神经网络的复杂性，想要计算所有参数的导数很复杂，提出了反向传播的方式计算导数

- 过拟合问题，加入正则化
  $$
  \displaystyle J(\theta) = \frac{1}{2m}\left[\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^n\theta_j^2\right]
  $$

  - 权值衰减，每次以一个小因子来衰减权值，加入一个与网络权值总量相关的惩罚项
  - 额外提供验证数据
  - K-fold交叉验证，把m个实例分成k个子集，然后交叉验证。

### 遗传算法

研究的问题： 搜索假设空间并确定最优假设，这里最优假设是使得适应度(fitness)最优的假设，例如(但不限于)，给定未知函数的输入输出，目标是逼近这个函数，那么相对这个函数的精度也就是我们的适应度

群体就是假设池， 每代计算满足最优假设最好的一些个体，用==概率==的方式选择这些个体，让这些个体生成新的一代

![](/Users/oliver/Desktop/AI_photo/GA.png)

这个算法核心步骤有

> 1. 选择：按照适应度大小按概率，保留上一代（1-r )\*p 个体
> 2. 交叉：从上一代p 中选择 rp/2 个实例对用于交叉，生成 r*p 个,从而种群数不变
> 3. 变异：从1，2 生成的ps 中取 m *P 个随机取一位反(这只是变异的一种方式)
> 4. 更新群体：P = Ps
> 5. 计算当前群体个体支持度，如果存在支持度大于给定的Max 则退出，否则重复2

==期望的证明==

==圆盘算法==

GABIL(表示假设)

GA 经常表示为二进制串，因此把每个属性数据转为二进制串，便于进行交叉和变异，二进制串的长度随属性种类的增加而增加

![](/Users/oliver/Desktop/AI_photo/GA.2.png)

> 这里编码方式是用每一位表示一个属性值，outlook 是三种属性，所以是三位，对于离散的数据，1 表示选择这个属性值，0表示不选这个属性值，例如这个规则没有设定outlook，隐含的也就是告诉我们 outlook = rain V sunny V cloudy，注意到wind 要求是wind  = strong 也就是确定不取另一个值'weak' ，因此我们得到10 

####基于实例学习

1. KNN，一个例子Xq，如何确定Xq的目标函数f(x)

   ​

### 观察与发现学习

- 概念聚类：

![](/Users/oliver/Desktop/AI_photo/概念聚类1.png)

![](/Users/oliver/Desktop/AI_photo/概念聚类2.png)

- 知识发现

  ![](/Users/oliver/Desktop/AI_photo/知识发现.png)

- 机器发现三大定律

  ![](/Users/oliver/Desktop/AI_photo/机器发现.png)

### 规则学习

- 序列覆盖算法，其实这个是之前GS ，AQ 的本质，都是不断的划出正例从而学习规则的方法
- 写FOIL算法

一阶Horn 子句：就是带变量的命题，这里有一些逻辑关系，相当于之前的命题形式吧规则写死了，horn子句用变量的形式表示通用规则 ，例如 Father('BOb','Bob1') 且 female('Bob1') -> daughter（true）=> father(y,x) 且female(y) -> daughter(true)

Foil算法是一个学习Horn子句的算法。FOCL是Foil的一个扩展，在学习过程中FOCL使用了领域知识。参照Foil算法的过程，给出FOCL算法

FOIL ：First Order Rule

![](/Users/oliver/Desktop/AI_photo/foil.png)

>  FOIL 和 FOCL 的关系
>
>  什么叫没有前件的谓词

### 贝叶斯学习

- 朴素贝叶斯分类器

### 强化学习

1. 马尔科夫决策过程
2. 写出**消极学习**的ID3算法
3. Q 值学习算法

给出“增强学习”中学习Q值的算法

### 课程总结

1. 给这门课的老师说，不要纸上谈兵!!!!!!