### 模型评价

> 计划综合西瓜书+ nn4nlp 完整调研模型检测

- ​

交叉验证(cross validation)

- 评估方法

  理想方案是对候选模型的泛化误差进行评估

  1. 留一法(Leave-one out)交叉验证:每次训练时把一个样本取出,将其他样本作为训练集,用训练好的模型对那个样本评价,这样就评价n 次,这n 次的准确率就表示为整个作为训练集的准确率

     特点适合训练集合非常小,例如只有100个

  2. 留出发(Held-out set):将样本直接划分为训练集和测试集,通常选择样本的2/3 ~ 4/5 作为训练集

     ~~~Python
      import numpy as np
      from sklearn.model_selection import train_test_split
      from sklearn import datasets
      from sklearn import svm

      iris = datasets.load_iris()
      iris.data.shape, iris.target.shape
     ((150, 4), (150,))
     X_train, X_test, y_train, y_test = train_test_split(
     ...     iris.data, iris.target, test_size=0.4, random_state=0)

      X_train.shape, y_train.shape
     ((90, 4), (90,))
      X_test.shape, y_test.shape
     ((60, 4), (60,))

      clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
      clf.score(X_test, y_test)                           
     0.96...
     ~~~

     问题是有时候要设置分类器 的参数如C ，这样可能把测试集合信息泄露到训练中，不能严格保证泛化性

  3. 交叉验证:将样本集合D分层划分为k 个集合,每次选择k-1个子集的并集 作为训练集合,剩下一个作为验证集,这样得到k 组训练集+验证集,最终结果取k 组的均值,这种称为 **k折交叉验证**

  ~~~python


  ~~~

  Data transformation with held out data 数据归一化 preprocessing

  可以自己设置想要用的 score 例如 f1_macro，precision_macro，recall_macro

  1. three-way split: 对训练多个模型

   ==概念理解==

  交叉验证其实并不是根据不同的划分去学习不同的模型，而是测试这个“使用全部训练集的模型”相比其他模型是否有优势，或者模型本身是否更有泛化能力，这k 个重新训练的模型都不会留存下来，仅仅用作评估

- 性能度量

1. **准确率** **(Aaccuracy)**: 分类器对整个样本的判定能力,即将正的判定为正，负的判定为负: A = (TP + TN)/ (TP + FN + FP + TN)

   > a、真正(True Positive , **TP**)：被模型预测为正的正样本
   >
   > b、假正(False Positive , **FP**)：被模型预测为正的负样本
   >
   > c、假负(False Negative , **FN**)：被模型预测为负的正样本
   >
   > d、真负(True Negative , **TN**)：被模型预测为负的负样本

   | 真实情况 | 预测结果 |      |
   | :--: | :--: | :--: |
   |      |  正例  |  反例  |
   |  正例  |  TP  |  FN  |
   |  反例  |  FP  |  TN  |

   ​

2. 查全率,召回率

   R = TP/(TP+FN)

3. 查准率，**精确率**(precision)

   P = TP/(TP+FP)

实验结果:

|                  | TFidf+word2vec | mean+word2vec  | onehot        |
| ---------------- | -------------- | -------------- | ------------- |
| has stopword     | 0.777142857143 | 0.785714285714 |               |
| without stopword | 0.771946564885 | 0.781488549618 | 0.68893129771 |

> 对tfidf 准确率下降的原因,其实在计算tfidf时的预料来自训练数据,这时出现一个问题,就是训练数据中大多是这个问题下的对应回复正确的标记值,这样使用tfidf时真正的主题词例如 '研究' '网络' 的tfidf 的数值就很低,这样造成加权的效果不好
>
> ont hot 有大量 0  的结果

> 资料
>
> [模型评估选择--来自西瓜书](http://bealin.github.io/2017/02/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E2%80%942-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/)