# 项目准备

> 主要针对暑期实习简历上项目描述

### 前言

因为大多数面试官会依次针对简历中的项目展开提问，因此项目中穿插技术点尤为重要，这也是指导机器学习，深度学习，知识点复习的重点，需要想的很清楚以下几点

1. 项目的流程是什么，目的
2. 怎么做的，为什幺这么做，这么做好处是什么，缺陷是什么
3. 遇到了那些难点，是怎么解决的



### Mooc 讨论区主观题自动评分项目

***

###项目背景

**背景**：在mooc 场景下，现有的技术多少通过静态的用户点击，观看视频的频次进行分析，很难对主观题对学生的回答进行评分。

**要解决的问题**：给定一个任意领域的问题文本和多个回复文本，已知假设有完全的标准答案，判断回答的准确性。准确性的粒度分为三种，2-way （正确，错误），3-way (正确，相反，错误)，5-way(正确，部分正确，相反，不相关，不在领域内)

**数据**：Semeval 2013 “The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge”

可能使用到的技术：

text classification, latent semantic analysis and (语义相似度)other semantic similarity measurements, textual entailment

###项目难点

1. 输入的问题不限制领域，不能使用领域内的知识库，但一定会使用到一些常识的概念，只能使用像词向量，wordnet 
2. 输入的文本的长度可变
3. 可获取的信息少：因为面向的是短文本，相对于机器阅读，从文本中抽取的信息有限
4. 标注数据获取难

###项目思路

#### 文本聚类(无监督)

> 这里是在获得标记数据之前的方法

1. 文本预处理

   - 去除标点，去除停用词
   - 分词

2. 获得句子的语义表示

   - 先是用词带模型VSM

3. 抽取特征，获得回复文本的特征向量

   词特征也就是 TF-IDF

3. **K-means 做文本聚类**。再请人工对每个类别进行评分，最终得到回复的分级

   特征选择：TF-IDF 加权 

   K-meas 的距离度量就是两个句子之间的相似度计算

   Mini-kmeans: 为了减少k-means 计算所有距离再更新质心这一问题，每次只给定部分数据就更新质心，

   > 这里也有主动学习self-learning,co-training 的意味，把
   >
   > [k-means](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html)

- 遇到的问题

  - 一句话描述k-means 的原理

    k-means 缺点：

    1. 簇的形状有偏向性，偏向球形

    2. k值

    3. 对离群点敏感: 离群点检测，局部异常因子(LOF)  算法

       > LOF: 通过比较每个点p和其邻域点的密度来判断该点是否为异常点，通过k-距离(p点最近的k个点)，如果点p的密度越低，越可能被认定是异常点

    4. 最初随机选择的k 个点对结果影响比较大,收敛慢的问题，使用 k-means ++ 

       > k-means ++ 直觉是随机选的中心之间分散是好事，第一个点随机选，

    5. 因为每次跟新权值 都要重新计算到质心的距离，可以使用minibatch 的方式，不用全集更新k-means 而是每次采样少量数据

  - k-means 需要指定k 值

  - k-means 实现的话需要使用什么数据结构？

  - k-means 如何进行评价

  ~~~python
  #sklearn 特征提取
  from sklearn import feature_extraction    
  from sklearn.feature_extraction.text import TfidfTransformer    
  from sklearn.feature_extraction.text import CountVectorizer  
  #CountVectorizer 转化为VSM
  #TfidfTransformer 做tfidf 加权并归一化
  from sklearn.cluster import KMeans as km

  clf = km()
  clf.label_
  输出

  ~~~

  ​

https://liangyaorong.github.io/blog/2017/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%A4%A7%E7%9B%98%E7%82%B9/

####项目实践

#### 文本分类

1. 文本清洗：tokenized 删除停用词

2. 语义表示

   - VSM 模型：使用词袋模型+TF-IDF,即每个向量的加和表示一个文本

     ==TF-IDF== ，使用描述这个句子的主题词构成的词组成VSM 向量表示句子

     tf–idf 正则化

     > 什么是词袋模型，什么是持续词袋模型

   - N-gram 特征：词袋表示法有限制

   - **词向量模型**：对两个句子 $D_1:{w_1,w_2,\dots,w_n},D_2:{w_1,\dots,w_m}$,每个 w 是句子中的单个词向量，那么句子D1 和D2 的词向量可以表示为
     $$
     sim_{doc}(D_1,D_2)=\sum _{i=1}^{n}\sum_{j=1}^{m}cos(w_i,w_j)\\
     sim_{doc}(D_1,D_2)=(\sum _{i=1}^{n}w_i)\cdot(\sum_{j=1}^{m}w_j)
     $$

     实现使用gensim 工具

     >  在NN4nlp 一书中提到之前我使用一个句子的所有词向量加和取平均再乘以另一个句子的证明，好开心:cowboy_hat_face:
     >
     >  gensim

3. 特征抽取

   - 词汇特征：重叠词数

   - 相似度特征：回复文本和问题文本，回复文本和标准答案的相似度参数

     - cosine

     - Lesk

       词义消歧技术，wordnet

     - overlap (重叠词个数)：使用hashtable 

     - Jaccard相似度 ： 

     - Levenshtein距离(莱文斯坦距离)

     > 相似度手段 : http://wetest.qq.com/lab/view/276.html

4. 分类：

   分类算法选择：

   - logestic 回归
   - 决策树，随机森林
   - 朴素贝叶斯

5. ==遇到的问题== : 类别不平衡问题

   ​

####深度语义表示

困难： 

1. 之前的一系列方案判断句子相似度，都是基于词到词的重合，其实有一些表达可能很相近，当时正因为有 “not” 类似这种反转意义大不相同，这就需要从句子语义的角度上分析，这就有从语义match 的角度看问题和答案，答案和标准答案。
2. 回到起点：怎么表示一个句子，尝试使用CNN
3. 怎么用两个句子计算match程度? 怎么构造网络？
4. 拿什么数据进行训练，因为我们只是拿他作为工具，我们不能提供太多先验知识

####提升

1. 深度语义匹配：之前相似度计算只是根据字面上，无法获得深层次语义，会发现之前正确的准确挺高，但是召回比较低，很多是误判了，加入词向量的表示形式，以及深度文本匹配
2. 能否使用文本聚类当做特征呢


#### 实践





#### 总结

mooc ASAG项目功能:给定一个任意领域的问题文本和多个回复文本，已知假设有完全的标准答案，判断回答的准确性。

1. 文本聚类：当只有未标注的数据时，一个问题，6000条回复，使用BOW + TFIDF 加权表示句子，使用K-means 和mini K-means对文本进行聚类
2. 文本分类：调研到SemEval 2013 有标注数据，所以使用有监督的文本分类，特征选择，选择回答和问题，标准答案的重叠词个数，编辑距离，Lesk相似度，词向量+TFIDF加权计算cosine，构造决策树C4.5分类器，进行2-way 和5-way 分类
3. 基于深度学习的文本匹配(目前正在进行)：使用gensim获得句子中每个词的词向量，使用CNN 对两个句子建模，构造神经网对两个句子进行建模。

##面向任务对话系统

###百度自然语言处理部实习

####度秘天气模块开发与维护

1. 熟悉面向任务的对话机器人系统流程，分为语音识别，语言理解，对话管理，对话生成
2. 维护度秘天气客户端，主要功能是进行历史信息的管理以及各个服务模块的调用，作为对话管理的行为实施者，调用SLU解析服务，对话生成服务，执行来自Policy 的决策，同时为对话生成准备天气数据
3. 应对新需求，为了减少对天气API服务的依赖，加入Redis服务器临时存储实时的天气数据
4. 协助开发百度通用任务对话机器人unit平台，帮助开发了一个展示页面的语音接入

AIML （人工智能标记语言）



谈到实习期间遇到那些困难：

1. 在一次对百度query 进行挖掘时，对query 进行关键词过滤时，随手写了一个find 方法，导致脚本运行极慢，后来查查资料使用dictmatch ，当时刚去体会到时间效率的重要性

####数据清洗

Linux AWK 使用

#### 智能出行助手

功能：这款应用结合之前在百度实习经历，原理相对简单的完整串联了从语言理解，对话管理，对话生成，提供类似微信的web交互界面，支持文本输入和语音接入，能完成火车票实时车票数量查询，票价查询，火车信息(经过站点，到站时间等)，能进行多轮交互。

语言理解(NLU) 使用框架语义(frame 语义表示)：domain,intent,slot

1. 语言理解模块：调用Google 的API.AI，设置意图和模板
2. 对话管理模块：主要使用Frame-based 方法，对话管理分为对话决策(Policy)和多轮历史管理(Session)两个子模块，将对话建模成一个填槽的过程，**槽**就是多轮对话过程中将初步用户意图转化为明确用户指令所需要补全的信息，对话决策依靠XML脚本中编写的对话逻辑，也称触发器，有询问、澄清或确认等系统动作以及用户设定的动作。
3. 对话生成：主要是基于规则模板，接受应用层传来的火车数据以及用户槽位数据进行语言生成，同样使用XML中配置相应模板





[对话管理](http://www.shuang0420.com/2018/01/03/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86(Dialog%20Management)/)

