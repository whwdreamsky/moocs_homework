# Perceptron Algorithm 

> from Graham Neubig 's tutorial 3

>  perceptron 意思是感知,在这里是什么意思呢
>
>  perceptron 感知机是SVM 支持向量机和神经网络的基础

### 感知机

定义:假设输入空间(特征空间)是 $X \subseteq R^n $,输出空间是$y= \{+1,-1\}$,其中对于$x\subseteq X$,x是特征向量,y 表示实例中的类别,满足如下函数,成为感知机.
$$
f(x) = sign(w\cdot x + b)
$$
其中w 便是特征x 对应的权重,也就是模型需要优化的对象,b 为偏置,这里sign 是符号函数
$$
sign = \lbrace \begin{equation}+1-1\rbrace \end{equation}
$$

> 这里感知机的目的就是求出训练数据的一个线性可分的超平面S , 感知机模型是判别模型



### 感知机算法

为了求得线性可分的超平面,则需要对误分类点到当前平面的距离优化,使之最小,下面求得误分类点到超平面距离,并通过运算去绝对值以及超平面距离之和作为损失函数
$$
d =\frac{1}{||w||}|w\cdot x +b|  \\
d_{simplify} = -\frac{1}{||w||}yi(w\cdot x +b) \\\
D_{sum} = -\frac{1}{||w||}\sum_{x\subseteq M}y_i(w\cdot x +b)
$$

> 这里表示的是**误**分类点的距离,误分类值的是 yi 与 wx0+b 预测的值不相同,进而得到符号不同,从而我们可以得到一个简化形式,其中 $w\cdot x​$ 表示w 和x 的内积,这里注意公式中 $x\subseteq M​$ 其实表示的是每次随机选取的误分类点集合,例如想作业中每次只选择一个更新,那这里直接通过 y(wx+b) 对参数进行更新

感知机算法: 通过对误分类点进行驱动,对所有误分类点到超平面的距离作为损失函数,首先任选一个超平面w0,b0,通过随机下降梯度,求解距离最小值,进而获得模型参数w,b

> ==公式里不是要计算所有点距离之和吗,这难道不是要求所有训练数据吗,其实这里的随机下降函数是指的是误分类点是随机给的,不是一次性给了全部的误分类点== 没弄懂

通过增广 即 wx+b -> WX,同时忽略|w|,我们得到损失函数如下:
$$
L = -\sum_{x\subseteq M}y_i(W\cdot X)
$$
参数优化:

 如果我们每次随机选择 *$\color{blue}{一个}$*误分类点通过对特征进行梯度优化,我们得到,优化方法如下;
$$
W \rightarrow W-(-\alpha yi\cdot X) \rightarrow W+\alpha yi\cdot X
$$

> 因为是求最小值,同时是向负梯度的方向移动,这里联想到之前Neubig 中通过y*权值 跟新权值

步骤;

1. 给出初值 w0,b0
2. 从训练数据中选择 x i ,y i 计算 $y_i(w_0x_i+b_0)$ ,如果小于0 进行步骤3,如果大于0 重复此步骤
3. 对参数进行更新 w -> w+alpha * yi *X
4. 进入2 知道没有误分类点退出,即误分类点遍历完成





### 收敛性证明

证明可以通过有限步的梯度下降,最终得到线性可分的平面,R 表示 max||$x_i$||,r 表示 y(wx+b) 最小值, 有无穷多个解,由于初值,迭代顺序的不同.
$$
k< (\frac {R}{r})^2
$$

### 代码实践

实现一个使用unigram 判断句子的极性,例如是否在讲人或者 情感正负,很多这种二分类问题

> 联想到我我之前的一个任务,对问题分类是否相关,也是一个二分类问题,有如下几个问题
>
> 1. 是否用判断这个数据集是线性可分的?
> 2. 感知机是通过误分类点进行判断,而我的问题更倾向与使用正例的样本去学习,因为正例可能特征是集中的 *当然这里值得商榷,到底是不是不确定*
> 3. 这里的特征可以有加入两个,两个相似度



### 多分类

### 感知机在神经网路

![perce_1](/home/oliver/Documents/moocs_homework/photo/perce_1.png)





龚恒代码学习

问题: 

1. 为什么要舍弃低频词,我测试了threshold =0,1 0 反而还高点

### 点到直线距离公式证明

点x0,y0 到点 y= wx+b 的距离公式如下
$$
d =\frac{1}{||w||}|w\cdot x +b|
$$
证明:面积法

推导一（面积法）：

![v2-859f9cb58d8af97af44cf0b60b9eaac9_hd](/home/oliver/Documents/moocs_homework/photo/v2-859f9cb58d8af97af44cf0b60b9eaac9_hd.jpg)

如上图所示,设 $R(x_r,y_0),S(x_0,y_s)$,设L 为Ax+By+C=0,有R,S 在直线上得到下列方程,由此可以得到xr ,ys 的表示,
$$
Ax_r+By_0+C=0 \\
Ax_0+By_s+C=0
$$
将xr和ys带入面积公式,由三角形面积公式:
$$
d\cdot\sqrt{(y_0-y_s)^2+(x_0-x_r)^2} = (x_0-x_r)(y_0-y_s)
$$
最终得到d 的公式:
$$
d =\frac{1}{\sqrt {A^2+B^2}}|Ax_0+By_0+C|
$$


