# Overfitting

è¿‡æ‹Ÿåˆ:å½“å‚æ•°è¿‡å¤šæˆ–è€…å¯¹ç‰¹å¾ä½¿ç”¨è¿‡å¤šçš„é«˜é˜¶å¤šé¡¹å¼è¡¨ç¤ºæ—¶,å­¦ä¹ ç®—æ³•ä¼šå°½å¯èƒ½ç¬¦åˆè®­ç»ƒæ•°æ®,ä½†æ˜¯è¿‡åº¦çš„å­¦ä¹ ä¸å¹¿æ³›çš„ç‰¹å¾,å¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å˜å·®.

> Logestic å¤šé¡¹å¼å¦‚ä½•é€‰æ‹©?

![3-3](/Users/oliver/Desktop/AI_photo//photo/3-3.png)

è§£å†³æ–¹æ³•:

###å‡å°‘å˜é‡çš„æ•°ç›®

1. æ¨¡å‹é€‰æ‹©ç®—æ³•,å³åªé€‰æ‹©å½±å“ç»“æœçš„å› ç´ 

###æ­£åˆ™åŒ–(Regularization) 

æ€æƒ³:å¸Œæœ›åœ¨æŸå¤±å‡½æ•°åŠ å…¥é¢å¤–é¡¹é™åˆ¶å‚æ•°ï¼Œå¯ä»¥çœ‹åšå¯¹æŸå¤±å‡½æ•°çš„æƒ©ç½š

1. ä¿ç•™æ‰€æœ‰å‚æ•°,ä½†æ˜¯å‡å°æ¨¡å‹å‚æ•°$\theta$çš„å€¼

   ä¿®æ”¹æŸå¤±å‡½æ•°:
   $$
   \displaystyle J(\theta) = \frac{1}{2m}\left[\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^n\theta_j^2\right]
   $$
   $\lambda$çš„å€¼é€‰æ‹©é—®é¢˜,å¦‚æœé€‰æ‹©çš„è¿‡å¤§,ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆ,å› ä¸ºå½“å¾ˆå¤§æ—¶ å‚æ•°$\theta_j$çš„å€¼è¶‹è¿‘äº0,è¿™æ ·å¯¼è‡´ç›®æ ‡å‡½æ•°$h_\theta(x)=\theta_0$

   > è¿™é‡Œæ·»åŠ ä¸€ä¸ªå‚æ•°çš„å¹³æ–¹ç›¸åŠ é¡¹,ç›¸å½“äºåŠ äº†ä¸€ä¸ªæ–¹å·®,è¿™é‡Œé€šå¸¸ä¸åŠ å…¥$\theta_0$å› ä¸ºé€šå¸¸ä»–çš„å€¼å¾ˆå¤§,è€Œä¸”å¯¹ç»“æœçš„å½±å“å¾ˆå°
   >
   > ä½†æ˜¯ä¸ºä»€ä¹ˆæ˜¯å¹³æ–¹å‘¢? ç­” ä½¿ç”¨çš„L2 æ­£åˆ™åŒ–é¡¹

   ==L1 vs  L2 æ­£åˆ™åŒ–==

   - L1æ­£åˆ™åŒ–æ˜¯æŒ‡æƒå€¼å‘é‡ $w$ ä¸­å„ä¸ªå…ƒç´ çš„**ç»å¯¹å€¼ä¹‹å’Œ**ï¼Œé€šå¸¸è¡¨ç¤ºä¸º$||w||_1$

     ç‰¹ç‚¹ï¼šå°±æ˜¯åŠ å…¥äº†æƒå€¼çš„ä¸€ä¸ªæ±‚å’Œï¼Œä¼šè®©å¾ˆå¤šæƒå€¼ç­‰äº0ï¼Œè¿›è€Œå½¢æˆ**ç¨€ç–çŸ©é˜µ**ï¼Œä¾¿äºè¿›è¡Œç‰¹å¾é€‰æ‹©

   - L2æ­£åˆ™åŒ–æ˜¯æŒ‡æƒå€¼å‘é‡ $w$ ä¸­å„ä¸ªå…ƒç´ çš„**å¹³æ–¹å’Œç„¶åå†æ±‚å¹³æ–¹æ ¹**

     ç‰¹ç‚¹ï¼šä¸»è¦ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè®©æƒå€¼è¶‹è¿‘äº0ï¼Œä½†ä¸æ˜¯ç­‰äº0, å¯ä»¥çœ‹åˆ°ğŸ‘‡ çº¿æ€§å›å½’çš„å‚æ•°æ›´æ–°çš„å¼å­ï¼Œæ¯æ¬¡ $\theta$ éƒ½ä¹˜ä¸Šä¸€ä¸ª$1 - \alpha\frac{\lambda}{m}$ ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªå°äº1 çš„å› å­ï¼Œå› æ­¤ä¼šä½¿$\theta$ çš„å€¼ä¸æ–­å˜å°

   æ€»ç»“: L1 å’Œ L2 æ˜¯ä¸¤ç§æ­£åˆ™åŒ–çš„æ‰‹æ®µï¼Œéƒ½æ˜¯æŸå¤±å‡½æ•°ä¸ŠåŠ å…¥æƒ©ç½šé¡¹å¯¹å‚æ•°è¿›è¡Œçº¦æŸï¼Œä¸åŒçš„åœ°æ–¹æ˜¯L1æ˜¯å»å‚æ•°ç»å¯¹å€¼ä¹‹å’Œï¼Œä¼˜åŒ–è¿‡ç¨‹ä¼šæ˜¯å¾ˆå¤šæƒå€¼ç­‰äº0ï¼Œè¿›è€Œå½¢æˆç¨€ç–çŸ©é˜µï¼Œä¾¿äºç‰¹å¾é€‰æ‹©ï¼Œè€ŒL2 æ˜¯åŠ å…¥å¹³æ–¹é¡¹ï¼Œåœ¨è¿›è¡Œæ¢¯åº¦ä¸‹é™æ—¶ï¼Œå‚æ•°ä¼šä¹˜ä¸Šä¸€ä¸ªå°äº1 çš„å€¼ï¼Œè¿›è€Œå‡å°å‚æ•°çš„å€¼

   â€‹

2. çº¿æ€§å›å½’æ­£åˆ™åŒ–

   a. æ ¹æ®ä¹‹å‰æŸå¤±å‡½æ•°,ç»“åˆä¹‹å‰æ¢¯åº¦ä¸‹é™çš„å…¬å¼,å¾—åˆ°æ­£åˆ™åŒ–æ¢¯åº¦ä¸‹é™å…¬å¼å¦‚ä¸‹
   $$
   \begin{align*} & \text{Repeat}\ \lbrace \newline & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \rbrace \end{align*}\\\\
   \theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
   $$

   > è¿™é‡Œ$\theta_j$å˜æˆä¸€æ¬¡æ˜¯å› ä¸ºå¯¹theta æ±‚äº†åå¯¼,ä»è€Œè·å¾—å…³äºtheta çš„ç­‰å¼

   b. ä»ç­‰å¼çš„è§’åº¦è·å¾—å‚æ•°æ–¹ç¨‹
   $$
   X\cdot \theta+ \lambda \cdot L\cdot\theta= Y\\X^T \cdot X\cdot\theta = X^T\cdot Y
   $$

   $$
   \begin{align*}& \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline& \text{where}\ \ L = \begin{bmatrix} 0 & & & & \newline & 1 & & & \newline & & 1 & & \newline & & & \ddots & \newline & & & & 1 \newline\end{bmatrix}\end{align*}
   $$

   > è¿™é‡Œæœ‰ç–‘é—®,æŒ‰ç…§ä¹‹å‰çš„å®šä¹‰åº”è¯¥æ˜¯æˆ‘å†™çš„ç­‰å¼,æˆ‘æ˜ç™½ç­‰å¼å·¦ä¾§åŒä¹˜ä¸€ä¸ª$X^T$çš„ç›®çš„æ˜¯,ä¸ºäº†è®©theta è„±ç¦»å‡ºæ¥,åŒæ—¶æ„é€ ä¸€ä¸ªå¯ä»¥æ±‚é€†çŸ©é˜µçš„n\*nçŸ©é˜µ,è¿™é‡Œç»™çš„è§£é‡Šæ˜¯ *X ä¸ºm\*nçš„çŸ©é˜µ,ä¸”m<n æ­¤æ—¶é€†ä¸å­˜åœ¨,åŠ ä¸Šä¸€ä¸ª$\lambda\cdot L$,è¿™æ ·é€†å°±å­˜åœ¨*,è¿™é‡Œä¸ç†è§£

3. é€»è¾‘æ–¯åœ°å›å½’æ­£åˆ™åŒ–

   è·Ÿä¹‹å‰çº¿æ€§å›å½’ç›¸ä¼¼,åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥å‚æ•°å¹³æ–¹é¡¹

   cost fuction
   $$
   J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
   $$
   gradient
   $$
   \theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
   $$
   > æ³¨æ„è¿™é‡Œæ­£åˆ™åŒ–æœ‰è¦æ±‚,ä¸è¦åŠ å…¥$\theta_0$,è¿™ä¸ªé—®é¢˜åœ¨ex3çš„ä½œä¸šä¸­å‡ºé”™äº†,å…¬å¼é‡Œä¹Ÿå¼ºè°ƒäº†$\theta$ æ˜¯ä»0å¼€å§‹çš„

   ex2 ä½œä¸š

   ```matlab
   %æ³¨æ„è¿™é‡Œå¯¹theta0ä¸è¿›è¡Œæ­£åˆ™åŒ–,å› æ­¤è®©theat0 ç½®0
   regression_theta = theta;
   regression_theta(1,1)=0;
   grad = (1/m)*X'*(sigmoid(X*theta)-y)+regression_theta*lambda/m;
   J = (-1/m)*(y'*log(sigmoid(X*theta))+(1-y)'*log(1-sigmoid(X*theta)))+(lambda/(2*m))*(regression_theta'*regression_theta);
   ```

   â€‹

   â€‹

   ä¸ä½¿ç”¨regularied å³ lamda = 0, accuracy = 86.440678

   ![3-3](/Users/oliver/Desktop/AI_photo//photo/3-5.png)

      ä½¿ç”¨regularied:æ˜æ˜¾å¹³æ»‘ä¸€äº›,èƒ½æ›´å¥½çš„é€‚åº”ä¼°è®¡ accuracy = 83.1 lamda = 1

![3-3](/Users/oliver/Desktop/AI_photo//photo/3-4.png)

â€‹	 ä½¿ç”¨regularied : è®¾ç½®å¤§çš„lamda =100,ä¸èƒ½å¾ˆå¥½çš„æ‹ŸåˆåŸæ•°æ®

â€‹	![3-6](/Users/oliver/Desktop/AI_photo//photo/3-6.png)








