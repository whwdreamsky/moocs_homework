# Gadient Descent



### Adagrad

> adapt gradient 自适应的梯度下降

问题： 普通的梯度下降或者SGD，学习率都是固定的，但是我们知道在训练过程中，在不断接近收敛时，学习率应该要减小。

1. 简单思路：确定稳定下降后在减小学习率，每次迭代减小学习率，但是参数的变化不同，需要给不同的参数有不同的学习率
2. 自动选择学习率 ：



###Adam