# RNN

### DNN vs RNN

DNN 无法对时间序列进行建模，在DNN 和CNN 上 反向传播只能向上一层传播，而RNN 可以在隐层 i 中 不但向i+1 层传播，还向本身第i 层连接  ， 左图 x ($x_1,x_2,\cdots,x_n$) 是简单在MLP 上将加入了一条自连接的边，右侧是左图的展开形式，隐层节点和隐层节点顺序连接，可以看到，t+1 时刻是$x_t+1$ 输入和之前所有历史的积累。

![rnn](/Users/oliver/Desktop/AI_photo/rnn.jpg)

RNN 与 HMM

看到这个形式，看起来很像HMM，实际上RNN 没有HMM 的马尔科夫假设，即当前状态只跟上一个状态有关，但是RNN 没有这个假设

RNN的参数是跨时刻共享的。也就是说，对任意时刻t，$s_{t-1}$到$s_t$，和 $x_t $到$s_t$

> 这里共享参数其实不太理解，简单理解是 对t+1 时刻 求反向传播时需要看xt 和s t ，这样BP 就不是仅仅是层到层之间传播也是层内部的传播。

### 参数共享

相比CNN的卷积核也是一种参数共享(看CNN 的说明)，RNN 中

###双向RNN

解决 RNN 只能获得之前节点信息，双向RNN 是获得之前和之后的信息

### LSTM

解决梯度消失和梯度爆炸，之前RNN 在计算ht 的时候是把ht-1 作为子部分，构成一个复合函数，如f(4) = f(f(3))=f((f(2)))这样在隐层节点比较多的时候，RNN 对较早节点求导时时导数连乘进而导致梯度消失和梯度下降

忘记门：决定需要忘记的信息