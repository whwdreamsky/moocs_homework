<!-- $theme: default -->

<!-- $theme: default -->
---
# <center>2017 秋季学期期末总结 <center>
---
# 总结要点
## - 	Mooc 讨论区文本分析
## - 	学习内容
## -未来规划
---

# Mooc讨论区文本分析

## 问题定义

之前的定义：与以往的Ｍooc数据分析通过点击,观看视频时长等用户操作不同,本课题针对慕课中 `讨论区问题的回复` 进行文本分析,根据问题的回复判断学生对问题的理解情况,最终实现对该问题所有学生的理解情况进行打分

Semval13 的**THE STUDENT RESPONSE ANALYSIS TASK**任务：给出一个问题，多个已知的参考答案，一个学生回答，目标是为学生回答评级

---

#### ASAG（自动短回答评分）

![](/Users/oliver/Desktop/AI_photo/asag/asag.1.png)

---

# 数据集

使用 Dzikovska et al., 2010 提供的BEETLE II data 数据集2729 条数据

#### 评价标准

有三种打分方式，5-way,2-way,3-way，分别是不同粒度的划分，同时难度也随之递减

- 5-way:回答主要评价为5类，正确(correct),部分正确(partially_correct_incomplete)，不切题(irrelevant)，不在范围内(non_domain)，矛盾(contradictory)

- 3-way:正确(correct) , 矛盾(contradictory),不正确 (incorrect)

- 2-way:   正确 (correct),不正确 (incorrect)

---
# 数据集
| type | example |
| ---------------------------- | ------ |
|question| Explain why you got a voltage reading of 1.5 for terminal 1 and the positive terminal|
|reference|Terminal 1 and the positive terminal are separated by the gap|
| Correct| Because there was a gap between the positive battery terminal and terminal 1   |
| irrelevant     | Voltage is the difference between a positive and negative end on a batter    |
| non_domain                   | i do not understand     |
| contradictory                | here was a connection between terminal 1 and the positive terminal    |


---



# 数据类别分布

| type                         | number | Freq |
| ---------------------------- | ------ | ---- |
| Correct                      | 1157   | 0.42 |
| partially_correct_incomplete | 626    | 0.23 |
| irrelevant                   | 656    | 0.24 |
| non_domain                   | 86     | 0.03 |
| contradictory                | 204    | 0.07 |

---

# baseline 

本次实现选择，问题和学生回答，标准答案和学生回答的相似度指标作为特征，根据来自 perl 的Text::Similarity::Overlaps module 包，选择overlap，cosine ，F1 score，Lesk 四个相似度指标，抽取特征后使用C4.5 分类器分类。
~~~python
1. overlap_SR_RR ：这里取学生回答和标准回答的词重叠的最大值
2. overlap_SR_QE ：这里取学生回答和问题的词重叠
3. cosine_SR_RR:  这里取学生回答和标准回答的词袋cosine的最大值
4. cosine_SR_QE：这里取学生回答和问题的cosine值
5. F1 score_SR_RR：这里取学生回答和标准回答的F1的最大值
6. F1 score_SR_QE：这里取学生回答和问题的F1
7. Lesk score_SR_RR：学生回答和标准回答的 Lesk score的最大值
8. Lesk score_SR_QE：这里取学生回答和问题的F1
~~~

---
# 实验结果

| way   | precision_macro     | recall_macro        | f1_macro            |
| ----- | ------------------- | ------------------- | ------------------- |
| 2-way | 0.64531 | 0.641738 | 0.64076028 |
| 3-way | 0.45058 | 0.449061 | 0.44723339 |
| 5-way | 0.38895 | 0.425116 | 0.39918886 |



---

# 实现流程

![](/Users/oliver/Desktop/AI_photo/2017期末检查/流程mooc1.png)

--- 

# MatchZoo 深度文本匹配工具箱

matchzoo 是一个深度文本匹配的一个工具箱，它实现了现在最新的10个深度文本匹配模型，它能应用在转述，文本蕴含，问答，对话，信息检索上。

| Tasks                      | Text 1   | Text 2     | Objective              |
| -------------------------- | -------- | ---------- | ---------------------- |
| Paraphrase Indentification | string 1 | string 2   | classification         |
| Textual Entailment         | text     | hypothesis | classification         |
| Question Answer            | question | answer     | classification/ranking |
| Conversation               | dialog   | response   | classification/ranking |
| Information Retrieval      | query    | document   | ranking                |
---

# 学习内容

- 公开课
  - **Graham Neubig's Teaching**
    - [Programming Basics](http://www.phontron.com/slides/nlp-programming-en-00-intro.pdf)
    - [Unigram Language Models](http://www.phontron.com/slides/nlp-programming-en-01-unigramlm.pdf)
    -  [Bigram Language Models](http://www.phontron.com/slides/nlp-programming-en-02-bigramlm.pdf)
    -  [Word Segmentation](http://www.phontron.com/slides/nlp-programming-en-03-ws.pdf)
    - [Part-of-Speech Tagging with Hidden Markov Models](http://www.phontron.com/slides/nlp-programming-en-04-hmm.pdf)
    - [The Perceptron Algorithm](http://www.phontron.com/slides/nlp-programming-en-05-perceptron.pdf)
    - [Neural Networks](http://www.phontron.com/slides/nlp-programming-en-07-nn.pdf)
  - **Neural Networks for Nlp**
    - Predicting the Next Word in a Sentence
    - Models of Words:word2vec
    - Models of Sentences:CNN

---
# 学习内容
- 课程学习:机器学习 ,数据挖掘,自然语言处理,  人工智能,组合数学
- TeaEvening ： 做了神经网络模型，词向量word2vec，层次softmax加速的分享

---

# 未来规划

- 比较关注短文本的语义理解，希望能结合论文把MatchZoo 的模型的实现理解清楚
- 在MatchZoo 提供的模型的基础上，对ASAG 任务进行自己的优化，例如他都是给定两个串比较相似度，如何参照问题和参考答案两方面构建模型

